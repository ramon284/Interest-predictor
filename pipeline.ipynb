{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download videos for training\n",
    "\n",
    "Downloads videos as mp4 and saves the audio as mp3. mp4 may or may not contain audio due to the API being limited, though this should not matter (except for the small negative impact on file size). Currently videos are being downloaded in 720p since 1080p seems bugged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytube import YouTube \n",
    "  \n",
    "urls = [\"https://www.youtube.com/watch?v=Yt3-a9mExZg\", \"https://www.youtube.com/watch?v=cGt8bEcd9Ms\"]\n",
    "pathVideo = 'Data/Video'\n",
    "pathAudio = 'Data/Audio'\n",
    "\n",
    "for i, url in enumerate(urls):\n",
    "    yt = YouTube(url)\n",
    "    yt.streams.filter(abr=\"160kbps\", progressive=False).first().download(filename=\"Audio\"+str(i)+\".mp3\", output_path = pathAudio)\n",
    "    yt.streams.filter(res=\"720p\", progressive=False).first().download(filename=\"Video\"+str(i)+\".mp4\", output_path= pathVideo)\n",
    "##"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split video into frames\n",
    "\n",
    "Goes through each downloaded video. Creates a directory per video inside 'tempFrames' and then saves the frames here. We can set the amount of frames we want per minute. Note that file sizes of these images can balloon quickly, exceeding the filesize of the video itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succesful videocapture?: True\n",
      "Succesful videocapture?: True\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "FRAMES_PER_MINUTE = 10\n",
    "frame_counter = 60000 / FRAMES_PER_MINUTE\n",
    "pathVideo = 'Data/Video/'\n",
    "videoNames = []\n",
    "for filename in os.listdir(pathVideo):\n",
    "    f = os.path.join(pathVideo, filename)\n",
    "    if os.path.isfile(f): ## checking if it is a file\n",
    "        videoNames.append(filename)\n",
    "\n",
    "for video in videoNames:\n",
    "    if video == '.gitignore':\n",
    "        continue\n",
    "    path = pathVideo + video\n",
    "    outputPath = 'Data/tempFrames/' + video[:-4] + '/' ## make directory per video\n",
    "    if not os.path.exists(outputPath):\n",
    "        os.mkdir(outputPath)\n",
    "        print(f'Created new dir {outputPath}')\n",
    "    vidcap = cv2.VideoCapture(pathVideo + video)\n",
    "    success,image = vidcap.read()\n",
    "    print(f'Succesful videocapture?: {success}')\n",
    "    count = 0\n",
    "    while success:\n",
    "        cv2.imwrite(outputPath + \"%d_seconds.jpg\" % (count*(frame_counter/1000)), image)     # save frame as JPEG file   \n",
    "        count += 1\n",
    "        vidcap.set(cv2.CAP_PROP_POS_MSEC,(count*frame_counter))   \n",
    "        success,image = vidcap.read()\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segmenting faces/webcam feeds\n",
    "\n",
    "By segmenting individuals, we can create temporary folders containing the frames per individual participant. This might be neccessary depending on the emotion detection model that we apply. It seems better to avoid this step is possible, as it introduces an extra model (segmentation), and also requires the creation of more files which need to be operated on. This could be rather inefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (408027126.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [5]\u001b[1;36m\u001b[0m\n\u001b[1;33m    import py-feat\u001b[0m\n\u001b[1;37m             ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import feat\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video emotion detection\n",
    "\n",
    "Now we run an emotion detection model to acquire some score. For every frame, we should get some \"emotion scores\" per individual. We can then extract some values such as minimum, maximum and mean scores per person. We can eventually combine this into a final score. For this to be as accurate as possible we will measure audio later "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Some code here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Audio analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "989389ff9eba75a921c11db3023a6ababd9b37f4218bf23a4fffc6887a815610"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
