{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download videos for training\n",
    "\n",
    "Downloads videos as mp4 and saves the audio as mp3. mp4 may or may not contain audio due to the API being limited, though this should not matter (except for the small negative impact on file size). Currently videos are being downloaded in 720p since 1080p is inconsistent/bugged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yo\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'span'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Ramon\\Desktop\\AccentureApplicatie\\pipeline.ipynb Cell 3\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Ramon/Desktop/AccentureApplicatie/pipeline.ipynb#W2sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39myo\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Ramon/Desktop/AccentureApplicatie/pipeline.ipynb#W2sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m yt \u001b[39m=\u001b[39m YouTube(url)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Ramon/Desktop/AccentureApplicatie/pipeline.ipynb#W2sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m yt\u001b[39m.\u001b[39;49mstreams\u001b[39m.\u001b[39mfilter(abr\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m160kbps\u001b[39m\u001b[39m\"\u001b[39m, progressive\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\u001b[39m.\u001b[39mfirst()\u001b[39m.\u001b[39mdownload(filename\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mAudio\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m+\u001b[39m\u001b[39mstr\u001b[39m(i)\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.mp3\u001b[39m\u001b[39m\"\u001b[39m, output_path \u001b[39m=\u001b[39m pathAudio)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Ramon/Desktop/AccentureApplicatie/pipeline.ipynb#W2sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m yt\u001b[39m.\u001b[39mstreams\u001b[39m.\u001b[39mfilter(res\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m720p\u001b[39m\u001b[39m\"\u001b[39m, progressive\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\u001b[39m.\u001b[39mfirst()\u001b[39m.\u001b[39mdownload(filename\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mVideo\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m+\u001b[39m\u001b[39mstr\u001b[39m(i)\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.mp4\u001b[39m\u001b[39m\"\u001b[39m, output_path\u001b[39m=\u001b[39m pathVideo)\n",
      "File \u001b[1;32mc:\\Users\\Ramon\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pytube\\__main__.py:296\u001b[0m, in \u001b[0;36mYouTube.streams\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    291\u001b[0m \u001b[39m\"\"\"Interface to query both adaptive (DASH) and progressive streams.\u001b[39;00m\n\u001b[0;32m    292\u001b[0m \n\u001b[0;32m    293\u001b[0m \u001b[39m:rtype: :class:`StreamQuery <StreamQuery>`.\u001b[39;00m\n\u001b[0;32m    294\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    295\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_availability()\n\u001b[1;32m--> 296\u001b[0m \u001b[39mreturn\u001b[39;00m StreamQuery(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfmt_streams)\n",
      "File \u001b[1;32mc:\\Users\\Ramon\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pytube\\__main__.py:181\u001b[0m, in \u001b[0;36mYouTube.fmt_streams\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[39m# If the cached js doesn't work, try fetching a new js file\u001b[39;00m\n\u001b[0;32m    179\u001b[0m \u001b[39m# https://github.com/pytube/pytube/issues/1054\u001b[39;00m\n\u001b[0;32m    180\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 181\u001b[0m     extract\u001b[39m.\u001b[39;49mapply_signature(stream_manifest, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvid_info, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mjs)\n\u001b[0;32m    182\u001b[0m \u001b[39mexcept\u001b[39;00m exceptions\u001b[39m.\u001b[39mExtractError:\n\u001b[0;32m    183\u001b[0m     \u001b[39m# To force an update to the js file, we clear the cache and retry\u001b[39;00m\n\u001b[0;32m    184\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_js \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Ramon\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pytube\\extract.py:409\u001b[0m, in \u001b[0;36mapply_signature\u001b[1;34m(stream_manifest, vid_info, js)\u001b[0m\n\u001b[0;32m    400\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_signature\u001b[39m(stream_manifest: Dict, vid_info: Dict, js: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    401\u001b[0m     \u001b[39m\"\"\"Apply the decrypted signature to the stream manifest.\u001b[39;00m\n\u001b[0;32m    402\u001b[0m \n\u001b[0;32m    403\u001b[0m \u001b[39m    :param dict stream_manifest:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    407\u001b[0m \n\u001b[0;32m    408\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 409\u001b[0m     cipher \u001b[39m=\u001b[39m Cipher(js\u001b[39m=\u001b[39;49mjs)\n\u001b[0;32m    411\u001b[0m     \u001b[39mfor\u001b[39;00m i, stream \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(stream_manifest):\n\u001b[0;32m    412\u001b[0m         \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Ramon\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pytube\\cipher.py:43\u001b[0m, in \u001b[0;36mCipher.__init__\u001b[1;34m(self, js)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform_map \u001b[39m=\u001b[39m get_transform_map(js, var)\n\u001b[0;32m     38\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjs_func_patterns \u001b[39m=\u001b[39m [\n\u001b[0;32m     39\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mw+\u001b[39m\u001b[39m\\\u001b[39m\u001b[39m.(\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mw+)\u001b[39m\u001b[39m\\\u001b[39m\u001b[39m(\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mw,(\u001b[39m\u001b[39m\\\u001b[39m\u001b[39md+)\u001b[39m\u001b[39m\\\u001b[39m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     40\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mw+\u001b[39m\u001b[39m\\\u001b[39m\u001b[39m[(\u001b[39m\u001b[39m\\\"\u001b[39;00m\u001b[39m\\\u001b[39m\u001b[39mw+\u001b[39m\u001b[39m\\\"\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\\\u001b[39m\u001b[39m]\u001b[39m\u001b[39m\\\u001b[39m\u001b[39m(\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mw,(\u001b[39m\u001b[39m\\\u001b[39m\u001b[39md+)\u001b[39m\u001b[39m\\\u001b[39m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     41\u001b[0m ]\n\u001b[1;32m---> 43\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mthrottling_plan \u001b[39m=\u001b[39m get_throttling_plan(js)\n\u001b[0;32m     44\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mthrottling_array \u001b[39m=\u001b[39m get_throttling_function_array(js)\n\u001b[0;32m     46\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcalculated_n \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Ramon\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pytube\\cipher.py:411\u001b[0m, in \u001b[0;36mget_throttling_plan\u001b[1;34m(js)\u001b[0m\n\u001b[0;32m    408\u001b[0m plan_regex \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39mcompile(transform_start)\n\u001b[0;32m    409\u001b[0m match \u001b[39m=\u001b[39m plan_regex\u001b[39m.\u001b[39msearch(raw_code)\n\u001b[1;32m--> 411\u001b[0m transform_plan_raw \u001b[39m=\u001b[39m find_object_from_startpoint(raw_code, match\u001b[39m.\u001b[39;49mspan()[\u001b[39m1\u001b[39m] \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m)\n\u001b[0;32m    413\u001b[0m \u001b[39m# Steps are either c[x](c[y]) or c[x](c[y],c[z])\u001b[39;00m\n\u001b[0;32m    414\u001b[0m step_start \u001b[39m=\u001b[39m \u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mc\u001b[39m\u001b[39m\\\u001b[39m\u001b[39m[(\u001b[39m\u001b[39m\\\u001b[39m\u001b[39md+)\u001b[39m\u001b[39m\\\u001b[39m\u001b[39m]\u001b[39m\u001b[39m\\\u001b[39m\u001b[39m(c\u001b[39m\u001b[39m\\\u001b[39m\u001b[39m[(\u001b[39m\u001b[39m\\\u001b[39m\u001b[39md+)\u001b[39m\u001b[39m\\\u001b[39m\u001b[39m](,c(\u001b[39m\u001b[39m\\\u001b[39m\u001b[39m[(\u001b[39m\u001b[39m\\\u001b[39m\u001b[39md+)\u001b[39m\u001b[39m\\\u001b[39m\u001b[39m]))?\u001b[39m\u001b[39m\\\u001b[39m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'span'"
     ]
    }
   ],
   "source": [
    "from pytube import YouTube \n",
    "  \n",
    "urls = [\"https://www.youtube.com/watch?v=Yt3-a9mExZg\", \"https://www.youtube.com/watch?v=cGt8bEcd9Ms\", 'https://www.youtube.com/watch?v=w00ooES3D5Y']\n",
    "urls = ['https://www.youtube.com/watch?v=w00ooES3D5Y']\n",
    "pathVideo = 'Data/Video'\n",
    "pathAudio = 'Data/Audio'\n",
    "\n",
    "for i, url in enumerate(urls):\n",
    "    print('yo')\n",
    "    yt = YouTube(url)\n",
    "    yt.streams.filter(abr=\"160kbps\", progressive=False).first().download(filename=\"Audio\"+str(i)+\".mp3\", output_path = pathAudio)\n",
    "    yt.streams.filter(res=\"720p\", progressive=False).first().download(filename=\"Video\"+str(i)+\".mp4\", output_path= pathVideo)\n",
    "##"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split video into frames\n",
    "\n",
    "Goes through each downloaded video. Creates a directory per video inside 'tempFrames' and then saves the frames here. We can set the amount of frames we want per minute. Note that file sizes of these images can balloon quickly, exceeding the filesize of the video itself.\n",
    "\n",
    "This code is mainly for early testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succesful videocapture?: True\n",
      "Succesful videocapture?: True\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "FRAMES_PER_MINUTE = 10\n",
    "frame_counter = 60000 / FRAMES_PER_MINUTE\n",
    "pathVideo = 'Data/Video/'\n",
    "videoNames = []\n",
    "for filename in os.listdir(pathVideo):\n",
    "    f = os.path.join(pathVideo, filename)\n",
    "    if os.path.isfile(f):\n",
    "        if filename != '.gitignore':\n",
    "            videoNames.append(filename)\n",
    "\n",
    "for video in videoNames:\n",
    "    path = pathVideo + video\n",
    "    outputPath = 'Data/tempFrames/' + video[:-4] + '/' ## make directory per video\n",
    "    if not os.path.exists(outputPath):\n",
    "        os.mkdir(outputPath)\n",
    "        print(f'Created new dir {outputPath}')\n",
    "    vidcap = cv2.VideoCapture(pathVideo + video)\n",
    "    success,image = vidcap.read()\n",
    "    print(f'Succesful videocapture?: {success}')\n",
    "    count = 0\n",
    "    while success:\n",
    "        cv2.imwrite(outputPath + \"%d_seconds.jpg\" % (count*(frame_counter/1000)), image)     # save frame as JPEG file   \n",
    "        count += 1\n",
    "        vidcap.set(cv2.CAP_PROP_POS_MSEC,(count*frame_counter))   \n",
    "        success,image = vidcap.read()\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Py-Feat\n",
    "\n",
    "This library provides a pipeline containing multiple models related to facial detection. The most interesting aspect here is the output of AU's (action units), which will be a key part of predicting interest levels.\n",
    "\n",
    "While the library works okay, it has difficulties with cuda and the documentation is lacking. The goal here is to use feat-py for prototyping and then later create a similar pipeline myself. This gives me more possibilities in for example fine-tuning, while also giving more control with regards to memory management and gpu usage. \n",
    "\n",
    "Action units cheat-sheet: https://py-feat.org/pages/au_reference.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import cv2\n",
    "import feat\n",
    "from feat.facepose_detectors.img2pose.img2pose_test import Img2Pose\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from Models.tracking_persistence.face_tracking_persistence import face_persistence_model\n",
    "from Preprocessing.videoCropper import videoCropper\n",
    "convert_tensor = transforms.ToTensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'detector' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m single_face_img_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(test_data_dir, \u001b[39m\"\u001b[39m\u001b[39msingle_face.jpg\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     10\u001b[0m \u001b[39m#single_face_prediction = detector.detect_image(single_face_img_path)\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m single_face_prediction \u001b[39m=\u001b[39m detector\u001b[39m.\u001b[39mdetect_image(\u001b[39m'\u001b[39m\u001b[39mData/testImages/ac3.jpg\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     13\u001b[0m \u001b[39m# Show results\u001b[39;00m\n\u001b[0;32m     14\u001b[0m single_face_prediction\u001b[39m.\u001b[39mplot_detections(au_barplot\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, emotion_barplot\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'detector' is not defined"
     ]
    }
   ],
   "source": [
    "from feat.utils.io import get_test_data_path\n",
    "from feat.plotting import imshow\n",
    "\n",
    "# Helper to point to the test data folder\n",
    "test_data_dir = get_test_data_path()\n",
    "\n",
    "# Get the full path\n",
    "single_face_img_path = os.path.join(test_data_dir, \"single_face.jpg\")\n",
    "\n",
    "#single_face_prediction = detector.detect_image(single_face_img_path)\n",
    "single_face_prediction = detector.detect_image('Data/testImages/ac3.jpg')\n",
    "\n",
    "# Show results\n",
    "single_face_prediction.plot_detections(au_barplot=False, emotion_barplot=False)\n",
    "single_face_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feat.utils.io import get_test_data_path\n",
    "import os\n",
    "\n",
    "test_data_dir = get_test_data_path()\n",
    "test_video_path = os.path.join(test_data_dir, \"WolfgangLanger_Pexels.mp4\")\n",
    "# test_video_path = \"Data/Video/Video1.mp4\"\n",
    "\n",
    "# Show video\n",
    "# from IPython.core.display import Video\n",
    "# Video(test_video_path, embed=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'openface'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Ramon\\Desktop\\AccentureApplicatie\\pipeline.ipynb Cell 10\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Ramon/Desktop/AccentureApplicatie/pipeline.ipynb#X14sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mopenface\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'openface'"
     ]
    }
   ],
   "source": [
    "import openface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_prediction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Face-tracker\n",
    "The class `face_persistence_model` is based on the `face_recognition` library, using the `face_locations` and `face_encoding` functions. `face_locations` searches a frame for faces and returns bounding box coordinates. `face_encoding` checks these faces to see if they can recognize the person. When it encounters a \"new\" face it saves some metadata. Then when detecting future faces it will check this new face against all the metadata files.\n",
    "\n",
    "Since the `face_persistence_model` serves as both a face detection and recognition model, it returns a dataframe consisting of multiple rows per frame, one row per detected face. The columns are `['Frame', 'Person', 'FaceRectX', 'FaceRectY', 'FaceRectWidth', 'FaceRectHeight']`. \n",
    "\n",
    "The inputs for the model are:\n",
    "1. `TOLERANCE`: A number [0,1] that alters how the model recognizes faces. A too low tolerance leads to the same face getting detected as different unique faces, too high tolerance leads to multiple faces being detected as the same person \n",
    "2. `filename`: The name of the videofile being analyzed. Must contain extension such as \".mp4\".\n",
    "3. `frameSkips`: This integer value represents how often a frame is skipped, i.e. if `frameSkips=3` every 3rd frame is skipped.\n",
    "4. `model`: Face location model choice of `['cnn', 'hog']`. While 'hog' is quicker (especially on CPU), 'cnn' is more accurate and utilizes GPU.\n",
    "5. `modelSize`: Size of the face encoding model, choice of `['small', 'large']`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m filename \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mVideo2_Trim_Trim.mp4\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m      6\u001b[0m face_detector_model \u001b[39m=\u001b[39m face_detector(filename\u001b[39m=\u001b[39mfilename, detection_threshold\u001b[39m=\u001b[39m\u001b[39m0.25\u001b[39m,)\n\u001b[1;32m----> 7\u001b[0m output \u001b[39m=\u001b[39m face_detector_model\u001b[39m.\u001b[39;49mrunModel()\n\u001b[0;32m      8\u001b[0m face_tracker \u001b[39m=\u001b[39m face_persistence_model(TOLERANCE\u001b[39m=\u001b[39m\u001b[39m0.6\u001b[39m, filename\u001b[39m=\u001b[39mfilename, model\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcnn\u001b[39m\u001b[39m'\u001b[39m, modelSize\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mlarge\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      9\u001b[0m output \u001b[39m=\u001b[39m face_tracker\u001b[39m.\u001b[39mrunDetectionTrackOnly(output, \u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\ramon.cremers\\OneDrive - Accenture\\Desktop\\AccentureApplicatie\\Models\\face_detection\\face_detection.py:26\u001b[0m, in \u001b[0;36mface_detector.runModel\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m     25\u001b[0m img \u001b[39m=\u001b[39m convert_tensor(img)\n\u001b[1;32m---> 26\u001b[0m pred \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mscale_and_predict(img)\n\u001b[0;32m     27\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(pred[\u001b[39m'\u001b[39m\u001b[39mboxes\u001b[39m\u001b[39m'\u001b[39m])):\n\u001b[0;32m     28\u001b[0m     top \u001b[39m=\u001b[39m pred[\u001b[39m'\u001b[39m\u001b[39mboxes\u001b[39m\u001b[39m'\u001b[39m][i][\u001b[39m1\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\ramon.cremers\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\feat\\facepose_detectors\\img2pose\\img2pose_test.py:160\u001b[0m, in \u001b[0;36mImg2Pose.scale_and_predict\u001b[1;34m(self, img, euler)\u001b[0m\n\u001b[0;32m    157\u001b[0m     scale \u001b[39m=\u001b[39m transformed_img[\u001b[39m\"\u001b[39m\u001b[39mScale\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m    159\u001b[0m \u001b[39m# Predict\u001b[39;00m\n\u001b[1;32m--> 160\u001b[0m preds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredict(img, border_size\u001b[39m=\u001b[39;49mborder_size, scale\u001b[39m=\u001b[39;49mscale, euler\u001b[39m=\u001b[39;49meuler)\n\u001b[0;32m    162\u001b[0m \u001b[39m# If the prediction is unsuccessful, try adding a white border to the image. This can improve bounding box\u001b[39;00m\n\u001b[0;32m    163\u001b[0m \u001b[39m# performance on images where face takes up entire frame, and images located at edge of frame.\u001b[39;00m\n\u001b[0;32m    164\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(preds[\u001b[39m\"\u001b[39m\u001b[39mboxes\u001b[39m\u001b[39m\"\u001b[39m]) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\ramon.cremers\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\feat\\facepose_detectors\\img2pose\\img2pose_test.py:188\u001b[0m, in \u001b[0;36mImg2Pose.predict\u001b[1;34m(self, img, border_size, scale, euler)\u001b[0m\n\u001b[0;32m    174\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Runs the img2pose model on the passed image and returns bboxes and face poses.\u001b[39;00m\n\u001b[0;32m    175\u001b[0m \n\u001b[0;32m    176\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    184\u001b[0m \n\u001b[0;32m    185\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    187\u001b[0m \u001b[39m# Obtain prediction\u001b[39;00m\n\u001b[1;32m--> 188\u001b[0m pred \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mpredict([img])[\u001b[39m0\u001b[39m]\n\u001b[0;32m    189\u001b[0m \u001b[39m# pred = self.model.predict(img)[0]\u001b[39;00m\n\u001b[0;32m    190\u001b[0m boxes \u001b[39m=\u001b[39m pred[\u001b[39m\"\u001b[39m\u001b[39mboxes\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()\u001b[39m.\u001b[39mastype(\u001b[39m\"\u001b[39m\u001b[39mfloat\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ramon.cremers\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\feat\\facepose_detectors\\img2pose\\img2pose_model.py:103\u001b[0m, in \u001b[0;36mimg2poseModel.predict\u001b[1;34m(self, imgs)\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfpn_model\u001b[39m.\u001b[39mtraining \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    102\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m--> 103\u001b[0m     predictions \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun_model(imgs)\n\u001b[0;32m    105\u001b[0m \u001b[39mreturn\u001b[39;00m predictions\n",
      "File \u001b[1;32mc:\\Users\\ramon.cremers\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\feat\\facepose_detectors\\img2pose\\img2pose_model.py:92\u001b[0m, in \u001b[0;36mimg2poseModel.run_model\u001b[1;34m(self, imgs, targets)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun_model\u001b[39m(\u001b[39mself\u001b[39m, imgs, targets\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m---> 92\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfpn_model(imgs, targets)\n\u001b[0;32m     93\u001b[0m     \u001b[39mreturn\u001b[39;00m outputs\n",
      "File \u001b[1;32mc:\\Users\\ramon.cremers\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\ramon.cremers\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\feat\\facepose_detectors\\img2pose\\img2pose_model.py:19\u001b[0m, in \u001b[0;36mWrappedModel.forward\u001b[1;34m(self, images, targets)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, images, targets\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m---> 19\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodule(images, targets)\n",
      "File \u001b[1;32mc:\\Users\\ramon.cremers\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\ramon.cremers\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\feat\\facepose_detectors\\img2pose\\deps\\generalized_rcnn.py:60\u001b[0m, in \u001b[0;36mGeneralizedRCNN.forward\u001b[1;34m(self, images, targets)\u001b[0m\n\u001b[0;32m     58\u001b[0m     original_image_sizes\u001b[39m.\u001b[39mappend((val[\u001b[39m0\u001b[39m], val[\u001b[39m1\u001b[39m]))\n\u001b[0;32m     59\u001b[0m images, targets \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform(images, targets)\n\u001b[1;32m---> 60\u001b[0m features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbackbone(images\u001b[39m.\u001b[39;49mtensors)\n\u001b[0;32m     61\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(features, torch\u001b[39m.\u001b[39mTensor):\n\u001b[0;32m     62\u001b[0m     features \u001b[39m=\u001b[39m OrderedDict([(\u001b[39m\"\u001b[39m\u001b[39m0\u001b[39m\u001b[39m\"\u001b[39m, features)])\n",
      "File \u001b[1;32mc:\\Users\\ramon.cremers\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\ramon.cremers\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torchvision\\models\\detection\\backbone_utils.py:57\u001b[0m, in \u001b[0;36mBackboneWithFPN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dict[\u001b[39mstr\u001b[39m, Tensor]:\n\u001b[1;32m---> 57\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbody(x)\n\u001b[0;32m     58\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfpn(x)\n\u001b[0;32m     59\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\ramon.cremers\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\ramon.cremers\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torchvision\\models\\_utils.py:69\u001b[0m, in \u001b[0;36mIntermediateLayerGetter.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     67\u001b[0m out \u001b[39m=\u001b[39m OrderedDict()\n\u001b[0;32m     68\u001b[0m \u001b[39mfor\u001b[39;00m name, module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems():\n\u001b[1;32m---> 69\u001b[0m     x \u001b[39m=\u001b[39m module(x)\n\u001b[0;32m     70\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturn_layers:\n\u001b[0;32m     71\u001b[0m         out_name \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturn_layers[name]\n",
      "File \u001b[1;32mc:\\Users\\ramon.cremers\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\ramon.cremers\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\pooling.py:166\u001b[0m, in \u001b[0;36mMaxPool2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    165\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor):\n\u001b[1;32m--> 166\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mmax_pool2d(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkernel_size, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[0;32m    167\u001b[0m                         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, ceil_mode\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mceil_mode,\n\u001b[0;32m    168\u001b[0m                         return_indices\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreturn_indices)\n",
      "File \u001b[1;32mc:\\Users\\ramon.cremers\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\_jit_internal.py:485\u001b[0m, in \u001b[0;36mboolean_dispatch.<locals>.fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    483\u001b[0m     \u001b[39mreturn\u001b[39;00m if_true(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    484\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 485\u001b[0m     \u001b[39mreturn\u001b[39;00m if_false(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ramon.cremers\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\functional.py:782\u001b[0m, in \u001b[0;36m_max_pool2d\u001b[1;34m(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)\u001b[0m\n\u001b[0;32m    780\u001b[0m \u001b[39mif\u001b[39;00m stride \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    781\u001b[0m     stride \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mannotate(List[\u001b[39mint\u001b[39m], [])\n\u001b[1;32m--> 782\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mmax_pool2d(\u001b[39minput\u001b[39;49m, kernel_size, stride, padding, dilation, ceil_mode)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from Models.tracking_persistence.face_tracking_persistence import face_persistence_model\n",
    "from Models.face_detection.face_detection import face_detector\n",
    "\n",
    "filename = 'Video2_Trim_Trim.mp4'\n",
    "\n",
    "face_detector_model = face_detector(filename=filename, detection_threshold=0.25,)\n",
    "output = face_detector_model.runModel()\n",
    "face_tracker = face_persistence_model(TOLERANCE=0.6, filename=filename, model='cnn', modelSize='large')\n",
    "output = face_tracker.runDetectionTrackOnly(output, False)\n",
    "face_tracker.saveCSV(output)\n",
    "videoCrop = videoCropper(output, filename, displayVideo=False, saveCSV=True)\n",
    "output = videoCrop.runModel()\n",
    "videoCrop.saveCSV()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video2_Trim_Trim.mp4\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video loaded? :  True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from Models.tracking_persistence.face_tracking_persistence import face_persistence_model\n",
    "from Models.face_detection.face_detection import face_detector\n",
    "from feat.au_detectors.StatLearning.SL_test import XGBClassifier\n",
    "filename = 'Video2_Trim_Trim.mp4'\n",
    "face_detector_model = face_detector(filename=filename, detection_threshold=0.25,)\n",
    "output = face_detector_model.runModel()\n",
    "face_tracker = face_persistence_model(TOLERANCE=0.6, filename=filename, model='cnn', modelSize='large')\n",
    "output = face_tracker.runDetectionTrackOnly(output, False)\n",
    "face_tracker.saveCSV(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(top)\n",
    "(output['FaceRectY'].value_counts())\n",
    "(df['FaceRectY'].value_counts()[459-top])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video segmentation\n",
    "\n",
    "To reduce memory usage it could be worthwile to only work with the video data inside of bounding boxes. I experimented with this in face_tracking_persistency.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Frame</th>\n",
       "      <th>Person</th>\n",
       "      <th>FaceRectX</th>\n",
       "      <th>FaceRectY</th>\n",
       "      <th>FaceRectWidth</th>\n",
       "      <th>FaceRectHeight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1307</th>\n",
       "      <td>438</td>\n",
       "      <td>0</td>\n",
       "      <td>888</td>\n",
       "      <td>40</td>\n",
       "      <td>94</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1308</th>\n",
       "      <td>438</td>\n",
       "      <td>2</td>\n",
       "      <td>271</td>\n",
       "      <td>138</td>\n",
       "      <td>113</td>\n",
       "      <td>114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1309</th>\n",
       "      <td>438</td>\n",
       "      <td>1</td>\n",
       "      <td>547</td>\n",
       "      <td>472</td>\n",
       "      <td>114</td>\n",
       "      <td>114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1310</th>\n",
       "      <td>439</td>\n",
       "      <td>0</td>\n",
       "      <td>888</td>\n",
       "      <td>40</td>\n",
       "      <td>94</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1311</th>\n",
       "      <td>439</td>\n",
       "      <td>2</td>\n",
       "      <td>259</td>\n",
       "      <td>138</td>\n",
       "      <td>114</td>\n",
       "      <td>114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1312</th>\n",
       "      <td>439</td>\n",
       "      <td>1</td>\n",
       "      <td>547</td>\n",
       "      <td>472</td>\n",
       "      <td>114</td>\n",
       "      <td>114</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Frame Person FaceRectX FaceRectY FaceRectWidth FaceRectHeight\n",
       "1307   438      0       888        40            94             95\n",
       "1308   438      2       271       138           113            114\n",
       "1309   438      1       547       472           114            114\n",
       "1310   439      0       888        40            94             95\n",
       "1311   439      2       259       138           114            114\n",
       "1312   439      1       547       472           114            114"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initialDF[1307:1313]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from feat import Detector\n",
    "#emoModel = 'svm'\n",
    "emoModel = 'resmasknet'\n",
    "mydet = Detector(emotion_model=emoModel)\n",
    "\n",
    "x_columns = [f'x{i}' for i in range(68)]\n",
    "y_columns = [f'y{i}' for i in range(68)]\n",
    "face_columns = ['FaceRectWidth', 'FaceRectHeight']\n",
    "bbox_columns = ['FaceRectX', 'FaceRectWidth', 'FaceRectY', 'FaceRectHeight']\n",
    "au_keys =      [\"AU1\",\"AU2\",\"AU4\",\"AU5\",\"AU6\",\"AU7\",\"AU9\",\"AU10\",\"AU11\",\"AU12\",\"AU14\",\n",
    "                \"AU15\",\"AU17\",\"AU20\",\"AU23\",\"AU24\",\"AU25\",\"AU26\",\"AU28\",\"AU43\"]\n",
    "emotion_keys = ['anger', 'disgust', 'fear',\t'happiness', 'sadness',\t'surprise',\t'neutral']\n",
    "auDF = pd.DataFrame(index=range(len(output)), columns=au_keys)\n",
    "emoDF = pd.DataFrame(index=range(len(output)), columns=emotion_keys)\n",
    "video = cv2.VideoCapture('./Data/Video/'+filename)\n",
    "frame_i = 0\n",
    "total_iterator = 0\n",
    "\n",
    "while True:\n",
    "    ret, image = video.read()\n",
    "    if ret == False:\n",
    "        break\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    grouped = output.groupby('Frame')\n",
    "    for frame, group in grouped:\n",
    "        if frame == frame_i:\n",
    "            frame_group = group\n",
    "            break\n",
    "    tempRowList = []\n",
    "    for _, row in frame_group.iterrows():\n",
    "        x_array = row[x_columns].tolist()\n",
    "        y_array = row[y_columns].tolist()\n",
    "        coordinates = list(zip(x_array, y_array))\n",
    "        coordinates = np.array(coordinates, dtype=np.float32)\n",
    "        nested_coordinates = [[coordinates]]\n",
    "        aus = mydet.detect_aus(image, nested_coordinates)\n",
    "        if emoModel == 'svm':\n",
    "            emotions = mydet.detect_emotions(frame=image, facebox=['a','b'], landmarks=nested_coordinates)\n",
    "        else:\n",
    "            faceInfo = row[bbox_columns].tolist()\n",
    "            faceInfo.append(1)\n",
    "            faceInfo = [int(x) for x in faceInfo]\n",
    "            faceInfo = [[faceInfo]]\n",
    "            emotions = mydet.detect_emotions(frame=image, facebox=faceInfo, landmarks=nested_coordinates)\n",
    "        for name, key in zip(au_keys,aus[0][0]):\n",
    "            auDF.loc[total_iterator, name] = key\n",
    "        for name, key in zip(emotion_keys, emotions[0][0]):\n",
    "            emoDF.loc[total_iterator, name] = key\n",
    "        total_iterator += 1\n",
    "    \n",
    "output2 = output.copy(deep=True)\n",
    "output2 = pd.concat([output2, auDF, emoDF], axis=1)\n",
    "output2.to_csv('final.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "989389ff9eba75a921c11db3023a6ababd9b37f4218bf23a4fffc6887a815610"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
