{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download videos for training\n",
    "\n",
    "Downloads videos as mp4 and saves the audio as mp3. mp4 may or may not contain audio due to the API being limited, though this should not matter (except for the small negative impact on file size). Currently videos are being downloaded in 720p since 1080p seems bugged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytube import YouTube \n",
    "  \n",
    "urls = [\"https://www.youtube.com/watch?v=Yt3-a9mExZg\", \"https://www.youtube.com/watch?v=cGt8bEcd9Ms\"]\n",
    "pathVideo = 'Data/Video'\n",
    "pathAudio = 'Data/Audio'\n",
    "\n",
    "for i, url in enumerate(urls):\n",
    "    yt = YouTube(url)\n",
    "    yt.streams.filter(abr=\"160kbps\", progressive=False).first().download(filename=\"Audio\"+str(i)+\".mp3\", output_path = pathAudio)\n",
    "    yt.streams.filter(res=\"720p\", progressive=False).first().download(filename=\"Video\"+str(i)+\".mp4\", output_path= pathVideo)\n",
    "##"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split video into frames\n",
    "\n",
    "Goes through each downloaded video. Creates a directory per video inside 'tempFrames' and then saves the frames here. We can set the amount of frames we want per minute. Note that file sizes of these images can balloon quickly, exceeding the filesize of the video itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succesful videocapture?: False\n",
      "Succesful videocapture?: True\n",
      "Succesful videocapture?: True\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "FRAMES_PER_MINUTE = 10\n",
    "frame_counter = 60000 / FRAMES_PER_MINUTE\n",
    "pathVideo = 'Data/Video/'\n",
    "videoNames = []\n",
    "for filename in os.listdir(pathVideo):\n",
    "    f = os.path.join(pathVideo, filename)\n",
    "    if os.path.isfile(f): ## checking if it is a file\n",
    "        videoNames.append(filename)\n",
    "\n",
    "for video in videoNames:\n",
    "    path = pathVideo + video\n",
    "    outputPath = 'Data/tempFrames/' + video[:-4] + '/' ## make directory per video\n",
    "    if not os.path.exists(outputPath):\n",
    "        os.mkdir(outputPath)\n",
    "        print(f'Created new dir {outputPath}')\n",
    "    vidcap = cv2.VideoCapture(pathVideo + video)\n",
    "    success,image = vidcap.read()\n",
    "    print(f'Succesful videocapture?: {success}')\n",
    "    count = 0\n",
    "    while success:\n",
    "        cv2.imwrite(outputPath + \"%d_seconds.jpg\" % (count*(frame_counter/1000)), image)     # save frame as JPEG file   \n",
    "        count += 1\n",
    "        vidcap.set(cv2.CAP_PROP_POS_MSEC,(count*frame_counter))   \n",
    "        success,image = vidcap.read()\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segmenting faces/webcam feeds\n",
    "\n",
    "By segmenting individuals, we can create temporary folders containing the frames per individual participant. This might be neccessary depending on the emotion detection model that we apply. It seems better to avoid this step is possible, as it introduces an extra model (segmentation), and also requires the creation of more files which need to be operated on. This could be rather inefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Some code here for extracting individuals, saving new cropped images"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video emotion detection\n",
    "\n",
    "Now we run an emotion detection model to acquire some score. For every frame, we should get some \"emotion scores\" per individual. We can then extract some values such as minimum, maximum and mean scores per person. We can eventually combine this into a final score. For this to be as accurate as possible we will measure audio later "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepface import DeepFace\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "face_model = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1151  277   64   64]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Action: emotion: 100%|██████████| 2/2 [00:00<00:00,  8.06it/s]\n",
      "Action: emotion: 100%|██████████| 2/2 [00:00<00:00,  8.08it/s]\n",
      "Action: emotion: 100%|██████████| 2/2 [00:00<00:00,  8.55it/s]\n",
      "Action: emotion: 100%|██████████| 2/2 [00:00<00:00,  8.28it/s]\n",
      "Action: emotion: 100%|██████████| 2/2 [00:00<00:00,  8.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'gender': {'Woman': 5.294562131166458, 'Man': 94.7054386138916}, 'dominant_gender': 'Man', 'region': {'x': 472, 'y': 545, 'w': 115, 'h': 115}, 'emotion': {'angry': 0.023690535410953933, 'disgust': 6.643793822520522e-06, 'fear': 99.4342624803938, 'happy': 4.84737114918727e-06, 'sad': 0.5307663082788174, 'surprise': 0.011244136782712074, 'neutral': 2.5825453183185254e-05}, 'dominant_emotion': 'fear'}, {'gender': {'Woman': 0.2635783748701215, 'Man': 99.73642230033875}, 'dominant_gender': 'Man', 'region': {'x': 928, 'y': 111, 'w': 110, 'h': 110}, 'emotion': {'angry': 1.4324935153126717, 'disgust': 0.022462835477199405, 'fear': 23.60522598028183, 'happy': 0.005210157178225927, 'sad': 74.85388517379761, 'surprise': 6.297289019130403e-05, 'neutral': 0.08065410074777901}, 'dominant_emotion': 'sad'}, {'gender': {'Woman': 0.00018886947827922995, 'Man': 99.99980926513672}, 'dominant_gender': 'Man', 'region': {'x': 914, 'y': 486, 'w': 114, 'h': 114}, 'emotion': {'angry': 2.2502200677990913, 'disgust': 0.8474414236843586, 'fear': 11.47925853729248, 'happy': 8.132655173540115, 'sad': 13.614726066589355, 'surprise': 16.132503747940063, 'neutral': 47.54319190979004}, 'dominant_emotion': 'neutral'}, {'gender': {'Woman': 0.02211132232332602, 'Man': 99.97789263725281}, 'dominant_gender': 'Man', 'region': {'x': 273, 'y': 101, 'w': 145, 'h': 145}, 'emotion': {'angry': 4.973066598176956, 'disgust': 4.165743661133092e-06, 'fear': 5.77203743159771, 'happy': 0.0018566197468317114, 'sad': 82.11588263511658, 'surprise': 0.004311347584007308, 'neutral': 7.132849097251892}, 'dominant_emotion': 'sad'}, {'gender': {'Woman': 0.4027486313134432, 'Man': 99.59724545478821}, 'dominant_gender': 'Man', 'region': {'x': 705, 'y': 534, 'w': 87, 'h': 87}, 'emotion': {'angry': 61.09859711519463, 'disgust': 0.000267696843860704, 'fear': 1.477833645959714, 'happy': 0.0016336777726471215, 'sad': 37.421324099381216, 'surprise': 7.065191075860741e-08, 'neutral': 0.00034829447288377795}, 'dominant_emotion': 'angry'}]\n",
      "[928 110 111 111]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Action: emotion: 100%|██████████| 2/2 [00:00<00:00,  7.96it/s]\n",
      "Action: emotion: 100%|██████████| 2/2 [00:00<00:00,  6.73it/s]\n",
      "Action: emotion: 100%|██████████| 2/2 [00:00<00:00,  8.40it/s]\n",
      "Action: emotion: 100%|██████████| 2/2 [00:00<00:00,  8.58it/s]\n",
      "Action: emotion: 100%|██████████| 2/2 [00:00<00:00,  8.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'gender': {'Woman': 5.294562131166458, 'Man': 94.7054386138916}, 'dominant_gender': 'Man', 'region': {'x': 472, 'y': 545, 'w': 115, 'h': 115}, 'emotion': {'angry': 0.023690535410953933, 'disgust': 6.643793822520522e-06, 'fear': 99.4342624803938, 'happy': 4.84737114918727e-06, 'sad': 0.5307663082788174, 'surprise': 0.011244136782712074, 'neutral': 2.5825453183185254e-05}, 'dominant_emotion': 'fear'}, {'gender': {'Woman': 0.2635783748701215, 'Man': 99.73642230033875}, 'dominant_gender': 'Man', 'region': {'x': 928, 'y': 111, 'w': 110, 'h': 110}, 'emotion': {'angry': 1.4324935153126717, 'disgust': 0.022462835477199405, 'fear': 23.60522598028183, 'happy': 0.005210157178225927, 'sad': 74.85388517379761, 'surprise': 6.297289019130403e-05, 'neutral': 0.08065410074777901}, 'dominant_emotion': 'sad'}, {'gender': {'Woman': 0.02211132232332602, 'Man': 99.97789263725281}, 'dominant_gender': 'Man', 'region': {'x': 273, 'y': 101, 'w': 145, 'h': 145}, 'emotion': {'angry': 4.973066598176956, 'disgust': 4.165743661133092e-06, 'fear': 5.77203743159771, 'happy': 0.0018566197468317114, 'sad': 82.11588263511658, 'surprise': 0.004311347584007308, 'neutral': 7.132849097251892}, 'dominant_emotion': 'sad'}, {'gender': {'Woman': 0.00018886947827922995, 'Man': 99.99980926513672}, 'dominant_gender': 'Man', 'region': {'x': 914, 'y': 486, 'w': 114, 'h': 114}, 'emotion': {'angry': 2.2502200677990913, 'disgust': 0.8474414236843586, 'fear': 11.47925853729248, 'happy': 8.132655173540115, 'sad': 13.614726066589355, 'surprise': 16.132503747940063, 'neutral': 47.54319190979004}, 'dominant_emotion': 'neutral'}, {'gender': {'Woman': 0.4027486313134432, 'Man': 99.59724545478821}, 'dominant_gender': 'Man', 'region': {'x': 705, 'y': 534, 'w': 87, 'h': 87}, 'emotion': {'angry': 61.09859711519463, 'disgust': 0.000267696843860704, 'fear': 1.477833645959714, 'happy': 0.0016336777726471215, 'sad': 37.421324099381216, 'surprise': 7.065191075860741e-08, 'neutral': 0.00034829447288377795}, 'dominant_emotion': 'angry'}]\n",
      "[271 100 148 148]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Action: emotion: 100%|██████████| 2/2 [00:00<00:00,  8.38it/s]\n",
      "Action: emotion: 100%|██████████| 2/2 [00:00<00:00,  8.48it/s]\n",
      "Action: emotion: 100%|██████████| 2/2 [00:00<00:00,  8.33it/s]\n",
      "Action: emotion: 100%|██████████| 2/2 [00:00<00:00,  8.59it/s]\n",
      "Action: emotion: 100%|██████████| 2/2 [00:00<00:00,  8.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'gender': {'Woman': 5.294562131166458, 'Man': 94.7054386138916}, 'dominant_gender': 'Man', 'region': {'x': 472, 'y': 545, 'w': 115, 'h': 115}, 'emotion': {'angry': 0.023690535410953933, 'disgust': 6.643793822520522e-06, 'fear': 99.4342624803938, 'happy': 4.84737114918727e-06, 'sad': 0.5307663082788174, 'surprise': 0.011244136782712074, 'neutral': 2.5825453183185254e-05}, 'dominant_emotion': 'fear'}, {'gender': {'Woman': 0.2635783748701215, 'Man': 99.73642230033875}, 'dominant_gender': 'Man', 'region': {'x': 928, 'y': 111, 'w': 110, 'h': 110}, 'emotion': {'angry': 1.4324935153126717, 'disgust': 0.022462835477199405, 'fear': 23.60522598028183, 'happy': 0.005210157178225927, 'sad': 74.85388517379761, 'surprise': 6.297289019130403e-05, 'neutral': 0.08065410074777901}, 'dominant_emotion': 'sad'}, {'gender': {'Woman': 0.02211132232332602, 'Man': 99.97789263725281}, 'dominant_gender': 'Man', 'region': {'x': 273, 'y': 101, 'w': 145, 'h': 145}, 'emotion': {'angry': 4.973066598176956, 'disgust': 4.165743661133092e-06, 'fear': 5.77203743159771, 'happy': 0.0018566197468317114, 'sad': 82.11588263511658, 'surprise': 0.004311347584007308, 'neutral': 7.132849097251892}, 'dominant_emotion': 'sad'}, {'gender': {'Woman': 0.00018886947827922995, 'Man': 99.99980926513672}, 'dominant_gender': 'Man', 'region': {'x': 914, 'y': 486, 'w': 114, 'h': 114}, 'emotion': {'angry': 2.2502200677990913, 'disgust': 0.8474414236843586, 'fear': 11.47925853729248, 'happy': 8.132655173540115, 'sad': 13.614726066589355, 'surprise': 16.132503747940063, 'neutral': 47.54319190979004}, 'dominant_emotion': 'neutral'}, {'gender': {'Woman': 0.4027486313134432, 'Man': 99.59724545478821}, 'dominant_gender': 'Man', 'region': {'x': 705, 'y': 534, 'w': 87, 'h': 87}, 'emotion': {'angry': 61.09859711519463, 'disgust': 0.000267696843860704, 'fear': 1.477833645959714, 'happy': 0.0016336777726471215, 'sad': 37.421324099381216, 'surprise': 7.065191075860741e-08, 'neutral': 0.00034829447288377795}, 'dominant_emotion': 'angry'}]\n",
      "[913 485 115 115]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Action: emotion: 100%|██████████| 2/2 [00:00<00:00,  8.68it/s]\n",
      "Action: emotion: 100%|██████████| 2/2 [00:00<00:00,  8.67it/s]\n",
      "Action: emotion: 100%|██████████| 2/2 [00:00<00:00,  7.75it/s]\n",
      "Action: emotion: 100%|██████████| 2/2 [00:00<00:00,  8.26it/s]\n",
      "Action: emotion: 100%|██████████| 2/2 [00:00<00:00,  8.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'gender': {'Woman': 0.2635783748701215, 'Man': 99.73642230033875}, 'dominant_gender': 'Man', 'region': {'x': 928, 'y': 111, 'w': 110, 'h': 110}, 'emotion': {'angry': 1.4324935153126717, 'disgust': 0.022462835477199405, 'fear': 23.60522598028183, 'happy': 0.005210157178225927, 'sad': 74.85388517379761, 'surprise': 6.297289019130403e-05, 'neutral': 0.08065410074777901}, 'dominant_emotion': 'sad'}, {'gender': {'Woman': 0.02211132232332602, 'Man': 99.97789263725281}, 'dominant_gender': 'Man', 'region': {'x': 273, 'y': 101, 'w': 145, 'h': 145}, 'emotion': {'angry': 4.973066598176956, 'disgust': 4.165743661133092e-06, 'fear': 5.77203743159771, 'happy': 0.0018566197468317114, 'sad': 82.11588263511658, 'surprise': 0.004311347584007308, 'neutral': 7.132849097251892}, 'dominant_emotion': 'sad'}, {'gender': {'Woman': 5.294562131166458, 'Man': 94.7054386138916}, 'dominant_gender': 'Man', 'region': {'x': 472, 'y': 545, 'w': 115, 'h': 115}, 'emotion': {'angry': 0.023690535410953933, 'disgust': 6.643793822520522e-06, 'fear': 99.4342624803938, 'happy': 4.84737114918727e-06, 'sad': 0.5307663082788174, 'surprise': 0.011244136782712074, 'neutral': 2.5825453183185254e-05}, 'dominant_emotion': 'fear'}, {'gender': {'Woman': 0.00018886947827922995, 'Man': 99.99980926513672}, 'dominant_gender': 'Man', 'region': {'x': 914, 'y': 486, 'w': 114, 'h': 114}, 'emotion': {'angry': 2.2502200677990913, 'disgust': 0.8474414236843586, 'fear': 11.47925853729248, 'happy': 8.132655173540115, 'sad': 13.614726066589355, 'surprise': 16.132503747940063, 'neutral': 47.54319190979004}, 'dominant_emotion': 'neutral'}, {'gender': {'Woman': 0.4027486313134432, 'Man': 99.59724545478821}, 'dominant_gender': 'Man', 'region': {'x': 705, 'y': 534, 'w': 87, 'h': 87}, 'emotion': {'angry': 61.09859711519463, 'disgust': 0.000267696843860704, 'fear': 1.477833645959714, 'happy': 0.0016336777726471215, 'sad': 37.421324099381216, 'surprise': 7.065191075860741e-08, 'neutral': 0.00034829447288377795}, 'dominant_emotion': 'angry'}]\n",
      "[469 543 116 116]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Action: emotion: 100%|██████████| 2/2 [00:00<00:00,  8.01it/s]\n",
      "Action: emotion: 100%|██████████| 2/2 [00:00<00:00,  5.21it/s]\n",
      "Action: emotion: 100%|██████████| 2/2 [00:00<00:00,  8.86it/s]\n",
      "Action: emotion: 100%|██████████| 2/2 [00:00<00:00,  8.02it/s]\n",
      "Action: emotion: 100%|██████████| 2/2 [00:00<00:00,  8.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'gender': {'Woman': 5.294562131166458, 'Man': 94.7054386138916}, 'dominant_gender': 'Man', 'region': {'x': 472, 'y': 545, 'w': 115, 'h': 115}, 'emotion': {'angry': 0.023690535410953933, 'disgust': 6.643793822520522e-06, 'fear': 99.4342624803938, 'happy': 4.84737114918727e-06, 'sad': 0.5307663082788174, 'surprise': 0.011244136782712074, 'neutral': 2.5825453183185254e-05}, 'dominant_emotion': 'fear'}, {'gender': {'Woman': 0.2635783748701215, 'Man': 99.73642230033875}, 'dominant_gender': 'Man', 'region': {'x': 928, 'y': 111, 'w': 110, 'h': 110}, 'emotion': {'angry': 1.4324935153126717, 'disgust': 0.022462835477199405, 'fear': 23.60522598028183, 'happy': 0.005210157178225927, 'sad': 74.85388517379761, 'surprise': 6.297289019130403e-05, 'neutral': 0.08065410074777901}, 'dominant_emotion': 'sad'}, {'gender': {'Woman': 0.02211132232332602, 'Man': 99.97789263725281}, 'dominant_gender': 'Man', 'region': {'x': 273, 'y': 101, 'w': 145, 'h': 145}, 'emotion': {'angry': 4.973066598176956, 'disgust': 4.165743661133092e-06, 'fear': 5.77203743159771, 'happy': 0.0018566197468317114, 'sad': 82.11588263511658, 'surprise': 0.004311347584007308, 'neutral': 7.132849097251892}, 'dominant_emotion': 'sad'}, {'gender': {'Woman': 0.00018886947827922995, 'Man': 99.99980926513672}, 'dominant_gender': 'Man', 'region': {'x': 914, 'y': 486, 'w': 114, 'h': 114}, 'emotion': {'angry': 2.2502200677990913, 'disgust': 0.8474414236843586, 'fear': 11.47925853729248, 'happy': 8.132655173540115, 'sad': 13.614726066589355, 'surprise': 16.132503747940063, 'neutral': 47.54319190979004}, 'dominant_emotion': 'neutral'}, {'gender': {'Woman': 0.4027486313134432, 'Man': 99.59724545478821}, 'dominant_gender': 'Man', 'region': {'x': 705, 'y': 534, 'w': 87, 'h': 87}, 'emotion': {'angry': 61.09859711519463, 'disgust': 0.000267696843860704, 'fear': 1.477833645959714, 'happy': 0.0016336777726471215, 'sad': 37.421324099381216, 'surprise': 7.065191075860741e-08, 'neutral': 0.00034829447288377795}, 'dominant_emotion': 'angry'}]\n",
      "[274 490 134 134]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Action: emotion: 100%|██████████| 2/2 [00:00<00:00,  8.73it/s]\n",
      "Action: emotion: 100%|██████████| 2/2 [00:00<00:00,  6.47it/s]\n",
      "Action: emotion: 100%|██████████| 2/2 [00:00<00:00,  7.51it/s]\n",
      "Action: emotion: 100%|██████████| 2/2 [00:00<00:00,  8.69it/s]\n",
      "Action: emotion: 100%|██████████| 2/2 [00:00<00:00,  8.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'gender': {'Woman': 5.294562131166458, 'Man': 94.7054386138916}, 'dominant_gender': 'Man', 'region': {'x': 472, 'y': 545, 'w': 115, 'h': 115}, 'emotion': {'angry': 0.023690535410953933, 'disgust': 6.643793822520522e-06, 'fear': 99.4342624803938, 'happy': 4.84737114918727e-06, 'sad': 0.5307663082788174, 'surprise': 0.011244136782712074, 'neutral': 2.5825453183185254e-05}, 'dominant_emotion': 'fear'}, {'gender': {'Woman': 0.2635783748701215, 'Man': 99.73642230033875}, 'dominant_gender': 'Man', 'region': {'x': 928, 'y': 111, 'w': 110, 'h': 110}, 'emotion': {'angry': 1.4324935153126717, 'disgust': 0.022462835477199405, 'fear': 23.60522598028183, 'happy': 0.005210157178225927, 'sad': 74.85388517379761, 'surprise': 6.297289019130403e-05, 'neutral': 0.08065410074777901}, 'dominant_emotion': 'sad'}, {'gender': {'Woman': 0.02211132232332602, 'Man': 99.97789263725281}, 'dominant_gender': 'Man', 'region': {'x': 273, 'y': 101, 'w': 145, 'h': 145}, 'emotion': {'angry': 4.973066598176956, 'disgust': 4.165743661133092e-06, 'fear': 5.77203743159771, 'happy': 0.0018566197468317114, 'sad': 82.11588263511658, 'surprise': 0.004311347584007308, 'neutral': 7.132849097251892}, 'dominant_emotion': 'sad'}, {'gender': {'Woman': 0.00018886947827922995, 'Man': 99.99980926513672}, 'dominant_gender': 'Man', 'region': {'x': 914, 'y': 486, 'w': 114, 'h': 114}, 'emotion': {'angry': 2.2502200677990913, 'disgust': 0.8474414236843586, 'fear': 11.47925853729248, 'happy': 8.132655173540115, 'sad': 13.614726066589355, 'surprise': 16.132503747940063, 'neutral': 47.54319190979004}, 'dominant_emotion': 'neutral'}, {'gender': {'Woman': 0.4027486313134432, 'Man': 99.59724545478821}, 'dominant_gender': 'Man', 'region': {'x': 705, 'y': 534, 'w': 87, 'h': 87}, 'emotion': {'angry': 61.09859711519463, 'disgust': 0.000267696843860704, 'fear': 1.477833645959714, 'happy': 0.0016336777726471215, 'sad': 37.421324099381216, 'surprise': 7.065191075860741e-08, 'neutral': 0.00034829447288377795}, 'dominant_emotion': 'angry'}]\n",
      "[703 533  89  89]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Action: emotion: 100%|██████████| 2/2 [00:00<00:00,  8.24it/s]\n",
      "Action: emotion: 100%|██████████| 2/2 [00:00<00:00,  7.94it/s]\n",
      "Action: emotion: 100%|██████████| 2/2 [00:00<00:00,  8.33it/s]\n",
      "Action: emotion: 100%|██████████| 2/2 [00:00<00:00,  8.52it/s]\n",
      "Action: emotion: 100%|██████████| 2/2 [00:00<00:00,  8.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'gender': {'Woman': 5.294562131166458, 'Man': 94.7054386138916}, 'dominant_gender': 'Man', 'region': {'x': 472, 'y': 545, 'w': 115, 'h': 115}, 'emotion': {'angry': 0.023690535410953933, 'disgust': 6.643793822520522e-06, 'fear': 99.4342624803938, 'happy': 4.84737114918727e-06, 'sad': 0.5307663082788174, 'surprise': 0.011244136782712074, 'neutral': 2.5825453183185254e-05}, 'dominant_emotion': 'fear'}, {'gender': {'Woman': 0.2635783748701215, 'Man': 99.73642230033875}, 'dominant_gender': 'Man', 'region': {'x': 928, 'y': 111, 'w': 110, 'h': 110}, 'emotion': {'angry': 1.4324935153126717, 'disgust': 0.022462835477199405, 'fear': 23.60522598028183, 'happy': 0.005210157178225927, 'sad': 74.85388517379761, 'surprise': 6.297289019130403e-05, 'neutral': 0.08065410074777901}, 'dominant_emotion': 'sad'}, {'gender': {'Woman': 0.02211132232332602, 'Man': 99.97789263725281}, 'dominant_gender': 'Man', 'region': {'x': 273, 'y': 101, 'w': 145, 'h': 145}, 'emotion': {'angry': 4.973066598176956, 'disgust': 4.165743661133092e-06, 'fear': 5.77203743159771, 'happy': 0.0018566197468317114, 'sad': 82.11588263511658, 'surprise': 0.004311347584007308, 'neutral': 7.132849097251892}, 'dominant_emotion': 'sad'}, {'gender': {'Woman': 0.00018886947827922995, 'Man': 99.99980926513672}, 'dominant_gender': 'Man', 'region': {'x': 914, 'y': 486, 'w': 114, 'h': 114}, 'emotion': {'angry': 2.2502200677990913, 'disgust': 0.8474414236843586, 'fear': 11.47925853729248, 'happy': 8.132655173540115, 'sad': 13.614726066589355, 'surprise': 16.132503747940063, 'neutral': 47.54319190979004}, 'dominant_emotion': 'neutral'}, {'gender': {'Woman': 0.4027486313134432, 'Man': 99.59724545478821}, 'dominant_gender': 'Man', 'region': {'x': 705, 'y': 534, 'w': 87, 'h': 87}, 'emotion': {'angry': 61.09859711519463, 'disgust': 0.000267696843860704, 'fear': 1.477833645959714, 'happy': 0.0016336777726471215, 'sad': 37.421324099381216, 'surprise': 7.065191075860741e-08, 'neutral': 0.00034829447288377795}, 'dominant_emotion': 'angry'}]\n"
     ]
    }
   ],
   "source": [
    "capture = cv2.VideoCapture('/Data/Video/Video0.mp4')\n",
    "length = int(capture.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "numberOfFrames = 10\n",
    "currentFrame = 0\n",
    "for i in range(12):\n",
    "    _, frame = capture.read()\n",
    "    if currentFrame != numberOfFrames:\n",
    "        currentFrame += 1\n",
    "        continue\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_model.detectMultiScale(gray, 1.1, 5)\n",
    "\n",
    "    for (x, y, w ,h) in faces:\n",
    "        cv2.rectangle(frame,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "\n",
    "    for face in faces:\n",
    "        print(face)\n",
    "        emotion = DeepFace.analyze(frame, actions = ['gender','emotion'])\n",
    "        print(emotion)\n",
    "\n",
    "    currentFrame = 0\n",
    "    cv2.imshow('x',frame)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "    cv2.waitKey(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Audio analysis\n",
    "\n",
    "#### Speech to text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset superb_demo (C:/Users/ramon.cremers/.cache/huggingface/datasets/anton-l___superb_demo/er/1.9.0/77d23894ff429329a7fe80f9007cabb0deec321316f8dda1a1e9d10ffa089d08)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import pipeline\n",
    "\n",
    "dataset = load_dataset(\"anton-l/superb_demo\", \"er\", split=\"session1\")\n",
    "\n",
    "classifier = pipeline(\"audio-classification\", model=\"superb/wav2vec2-base-superb-er\")\n",
    "labels = classifier(dataset[0][\"file\"], top_k=5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import librosa\n",
    "from datasets import load_dataset\n",
    "from transformers import Wav2Vec2ForSequenceClassification, Wav2Vec2FeatureExtractor\n",
    "\n",
    "def map_to_array(example):\n",
    "    speech, _ = librosa.load(example[\"file\"], sr=16000, mono=True)\n",
    "    example[\"speech\"] = speech\n",
    "    return example\n",
    "\n",
    "# load a demo dataset and read audio files\n",
    "dataset = load_dataset(\"anton-l/superb_demo\", \"er\", split=\"session1\")\n",
    "print(dataset)\n",
    "dataset = dataset.map(map_to_array)\n",
    "\n",
    "model = Wav2Vec2ForSequenceClassification.from_pretrained(\"superb/wav2vec2-base-superb-er\")\n",
    "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(\"superb/wav2vec2-base-superb-er\")\n",
    "\n",
    "# compute attention masks and normalize the waveform if needed\n",
    "inputs = feature_extractor(dataset[:4][\"speech\"], sampling_rate=16000, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "logits = model(**inputs).logits\n",
    "predicted_ids = torch.argmax(logits, dim=-1)\n",
    "labels = [model.config.id2label[_id] for _id in predicted_ids.tolist()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hap', 'hap', 'ang', 'hap']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "labels"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7a738a2c0553d3939d099c48e23577ff5602c66ff25881ba29098dd326541e04"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
