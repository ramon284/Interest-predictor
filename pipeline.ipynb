{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download videos for training\n",
    "\n",
    "Downloads videos as mp4 and saves the audio as mp3. mp4 may or may not contain audio due to the API being limited, though this should not matter (except for the small negative impact on file size). Currently videos are being downloaded in 720p since 1080p seems bugged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytube import YouTube \n",
    "  \n",
    "urls = [\"https://www.youtube.com/watch?v=Yt3-a9mExZg\", \"https://www.youtube.com/watch?v=cGt8bEcd9Ms\", \n",
    "        \"https://www.youtube.com/watch?v=rhzTeSrUynE\"]\n",
    "pathVideo = 'Data/Video'\n",
    "pathAudio = 'Data/Audio'\n",
    "\n",
    "for i, url in enumerate(urls):\n",
    "    yt = YouTube(url)\n",
    "    yt.streams.filter(abr=\"160kbps\", progressive=False).first().download(filename=\"Audio\"+str(i)+\".mp3\", output_path = pathAudio)\n",
    "    yt.streams.filter(res=\"720p\", progressive=False).first().download(filename=\"Video\"+str(i)+\".mp4\", output_path= pathVideo)\n",
    "##"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split video into frames\n",
    "\n",
    "Goes through each downloaded video. Creates a directory per video inside 'tempFrames' and then saves the frames here. We can set the amount of frames we want per minute. Note that file sizes of these images can balloon quickly, exceeding the filesize of the video itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succesful videocapture?: False\n",
      "Succesful videocapture?: True\n",
      "Succesful videocapture?: True\n",
      "Succesful videocapture?: True\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "FRAMES_PER_MINUTE = 10\n",
    "frame_counter = 60000 / FRAMES_PER_MINUTE\n",
    "pathVideo = 'Data/Video/'\n",
    "videoNames = []\n",
    "for filename in os.listdir(pathVideo):\n",
    "    f = os.path.join(pathVideo, filename)\n",
    "    if os.path.isfile(f): ## checking if it is a file\n",
    "        videoNames.append(filename)\n",
    "\n",
    "for video in videoNames:\n",
    "    path = pathVideo + video\n",
    "    outputPath = 'Data/tempFrames/' + video[:-4] + '/' ## make directory per video\n",
    "    if not os.path.exists(outputPath):\n",
    "        os.mkdir(outputPath)\n",
    "        print(f'Created new dir {outputPath}')\n",
    "    vidcap = cv2.VideoCapture(pathVideo + video)\n",
    "    success,image = vidcap.read()\n",
    "    print(f'Succesful videocapture?: {success}')\n",
    "    count = 0\n",
    "    while success:\n",
    "        cv2.imwrite(outputPath + \"%d_seconds.jpg\" % (count*(frame_counter/1000)), image)     # save frame as JPEG file   \n",
    "        count += 1\n",
    "        vidcap.set(cv2.CAP_PROP_POS_MSEC,(count*frame_counter))   \n",
    "        success,image = vidcap.read()\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segmenting faces/webcam feeds\n",
    "\n",
    "By segmenting individuals, we can create temporary folders containing the frames per individual participant. This might be neccessary depending on the emotion detection model that we apply. It seems better to avoid this step is possible, as it introduces an extra model (segmentation), and also requires the creation of more files which need to be operated on. This could be rather inefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Some code here for extracting individuals, saving new cropped images"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video emotion detection\n",
    "\n",
    "Now we run an emotion detection model to acquire some score. For every frame, we should get some \"emotion scores\" per individual. We can then extract some values such as minimum, maximum and mean scores per person. We can eventually combine this into a final score. For this to be as accurate as possible we will measure audio later "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepface import DeepFace\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "face_model = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capture = cv2.VideoCapture('Data\\Video\\Video2.mp4')\n",
    "\n",
    "length = int(capture.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "print(length)\n",
    "numberOfFrames = 500\n",
    "currentFrame = 0\n",
    "for i in range(2002):\n",
    "    _, frame = capture.read()\n",
    "    if currentFrame != numberOfFrames:\n",
    "        currentFrame += 1\n",
    "        continue\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_model.detectMultiScale(gray, 1.1, 5)\n",
    "\n",
    "    for (x, y, w ,h) in faces:\n",
    "        cv2.rectangle(frame,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "\n",
    "    for face in faces:\n",
    "        print(f'frame {i}')\n",
    "        print(face)\n",
    "        emotion = DeepFace.analyze(frame, actions = ['emotion'])\n",
    "        print(emotion[0]['dominant_emotion'])\n",
    "\n",
    "    currentFrame = 0\n",
    "    cv2.imshow('x',frame)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "    cv2.waitKey(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Audio analysis\n",
    "\n",
    "#### Speech to text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset superb_demo (C:/Users/ramon.cremers/.cache/huggingface/datasets/anton-l___superb_demo/er/1.9.0/77d23894ff429329a7fe80f9007cabb0deec321316f8dda1a1e9d10ffa089d08)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import pipeline\n",
    "\n",
    "dataset = load_dataset(\"anton-l/superb_demo\", \"er\", split=\"session1\")\n",
    "\n",
    "classifier = pipeline(\"audio-classification\", model=\"superb/wav2vec2-base-superb-er\")\n",
    "labels = classifier(dataset[0][\"file\"], top_k=5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset superb_demo (C:/Users/ramon.cremers/.cache/huggingface/datasets/anton-l___superb_demo/er/1.9.0/77d23894ff429329a7fe80f9007cabb0deec321316f8dda1a1e9d10ffa089d08)\n",
      "Loading cached processed dataset at C:\\Users\\ramon.cremers\\.cache\\huggingface\\datasets\\anton-l___superb_demo\\er\\1.9.0\\77d23894ff429329a7fe80f9007cabb0deec321316f8dda1a1e9d10ffa089d08\\cache-4a0c27eacd16263f.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['file', 'audio', 'label'],\n",
      "    num_rows: 6\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import librosa\n",
    "from datasets import load_dataset\n",
    "from transformers import Wav2Vec2ForSequenceClassification, Wav2Vec2FeatureExtractor\n",
    "\n",
    "def map_to_array(example):\n",
    "    speech, _ = librosa.load(example[\"file\"], sr=16000, mono=True)\n",
    "    example[\"speech\"] = speech\n",
    "    return example\n",
    "\n",
    "# load a demo dataset and read audio files\n",
    "dataset = load_dataset(\"anton-l/superb_demo\", \"er\", split=\"session1\")\n",
    "print(dataset)\n",
    "dataset = dataset.map(map_to_array)\n",
    "\n",
    "model = Wav2Vec2ForSequenceClassification.from_pretrained(\"superb/wav2vec2-base-superb-er\")\n",
    "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(\"superb/wav2vec2-base-superb-er\")\n",
    "\n",
    "# compute attention masks and normalize the waveform if needed\n",
    "inputs = feature_extractor(dataset[:4][\"speech\"], sampling_rate=16000, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "logits = model(**inputs).logits\n",
    "predicted_ids = torch.argmax(logits, dim=-1)\n",
    "labels = [model.config.id2label[_id] for _id in predicted_ids.tolist()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hap', 'hap', 'ang', 'hap']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7a738a2c0553d3939d099c48e23577ff5602c66ff25881ba29098dd326541e04"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
